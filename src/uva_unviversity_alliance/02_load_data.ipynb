{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load World Cup Data (Step 4)\n",
        "\n",
        "**Prerequisites:** Run `01_set_up.ipynb` first to create the catalog, schema, volume, and tables. Upload all CSV files from the `data/` folder to the volume `university_learning.world_cup.world_cup_data` before running this notebook.\n",
        "\n",
        "This notebook reads all CSV files from the volume and loads them into the Delta tables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "55645723-ac2d-44de-aa5b-b9cd0829745b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "# Load World Cup CSV data from the volume into Delta tables\n",
        "# Path to the volume created in 01_set_up.ipynb (Step 3)\n",
        "from pyspark.sql.functions import lit, current_timestamp\n",
        "\n",
        "world_cup_data_path = '/Volumes/university_learning/world_cup/world_cup_data/'\n",
        "files = dbutils.fs.ls(world_cup_data_path)\n",
        "csv_files = [file for file in files if file.name.endswith('.csv')]\n",
        "\n",
        "# Dictionary to store dataframes\n",
        "dataframes = {}\n",
        "\n",
        "for file in csv_files:\n",
        "    table_name = file.name.replace('.csv', '')\n",
        "    file_path = world_cup_data_path + file.name\n",
        "    table_full_name = f\"university_learning.world_cup.{table_name}\"\n",
        "    \n",
        "    print(f\"Reading {file.name}...\")\n",
        "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "    \n",
        "    # Add audit_update_ts column\n",
        "    df_to_write = df.withColumn('audit_update_ts', current_timestamp())\n",
        "    \n",
        "    # Write to Delta table\n",
        "    print(f\" Truncating and writing to {table_full_name}...\")\n",
        "    # Truncate before append to avoid duplicates when re-running this demo\n",
        "    spark.sql(f\"TRUNCATE TABLE {table_full_name}\")\n",
        "\n",
        "    df_to_write.write.mode(\"append\").saveAsTable(table_full_name)\n",
        "    \n",
        "    # Store original df for DDL generation\n",
        "    dataframes[table_name] = df\n",
        "    \n",
        "    row_count = df.count()\n",
        "    print(f\"  âœ“ {table_name}: {row_count} rows written\\n\")\n",
        "\n",
        "print(f\"\\nSuccessfully loaded and wrote {len(dataframes)} tables\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "4"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "02_load_data",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}